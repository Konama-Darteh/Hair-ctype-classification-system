# -*- coding: utf-8 -*-
"""CNN,HairNet and RF code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RSWNgbCAaUyk5XZXtt3gNCD5tBwJ02Uy
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import load_model

# For saving the model
import pickle
from google.colab import drive
drive.mount('/content/drive')

# Paths for the training and testing datasets
TRAIN_DIR = '/content/drive/MyDrive/Figaro-1k/Original'
TEST_DIR = '/content/drive/MyDrive/Figaro-1k/Original/Testing'

import os
# Step 1: Check filenames in TRAIN_DIR
print("Files in Training Directory:")
print(os.listdir(TRAIN_DIR))

import os
print(os.listdir('/content/drive/MyDrive/Figaro-1k/Original'))

"""NEW CHANGES USE THE FOLDER METHOD USED FOR THE TESTING DATASET TO LABEL THE IMAGES FOR TRANING."""

# Ensure directory exists
if os.path.exists(TRAIN_DIR):
    print(f"✅ Training directory found: {TRAIN_DIR}")
else:
    raise ValueError("❌ Training directory NOT found! Check the path.")

# Scan the Training directory for images inside subfolders
train_image_paths = []
train_labels = []

for root, dirs, files in os.walk(TRAIN_DIR):
    for file in files:
        if file.lower().endswith(('.jpg', '.jpeg', '.png')):
            file_path = os.path.join(root, file)
            label = os.path.basename(root)  # Extract folder name as label

            # Ensure we are not labeling everything as "Training"
            if label.lower() != "training":  # Skip "Training" if it's mistakenly picked
                train_image_paths.append(file_path)
                train_labels.append(label)

# Create a DataFrame
train_df = pd.DataFrame({"image_path": train_image_paths, "label": train_labels})

# Save to CSV
csv_path = "/content/drive/MyDrive/Figaro-1k/train_labels_fixed.csv"
train_df.to_csv(csv_path, index=False)

print(f"✅ Saved training labels CSV with {len(train_df)} entries.")
print(train_df.head())  # Display first few rows to check if labels are correct

#import label encoder
from sklearn.preprocessing import LabelEncoder

# Encode labels using LabelEncoder
label_encoder = LabelEncoder()
train_labels_encoded = label_encoder.fit_transform(train_labels)

# Save the label encoder for later use
with open('/content/drive/MyDrive/label_encoder.pkl', 'wb') as file:
    pickle.dump(label_encoder, file)

print("✅ Label Encoder saved.")

# One-hot encode labels (for model training)
import tensorflow as tf
train_labels_one_hot = tf.keras.utils.to_categorical(train_labels_encoded, num_classes=len(label_encoder.classes_))

print(f"✅ Encoded {len(train_labels)} training labels.")

"""After labeling the images, the CNN and Hairnet models are trained to categories the images based on common characteristics

Load and Preprocess Data
"""

# Load dataset
data = pd.read_csv('/content/drive/MyDrive/Figaro-1k/test_labels_fixed.csv')

print(data.columns)

# Extract image paths and labels
#image_paths = data['image_path']
#labels = data['predicted_label']

# Encode labels into integers
#from sklearn.preprocessing import LabelEncoder
#label_encoder = LabelEncoder()
#labels_encoded = label_encoder.fit_transform(labels)

import pickle

# Save the label encoder to a file
#with open('/content/drive/MyDrive/label_encoder.pkl', 'wb') as file:
    #pickle.dump(label_encoder, file)

#print("Label encoder saved successfully!")

# Data generator for loading images from directories
datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

# Image dimensions
IMG_HEIGHT, IMG_WIDTH = 128, 128

train_generator = datagen.flow_from_directory(
    TRAIN_DIR,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=32,
    class_mode='categorical',
    subset='training'
)

val_generator = datagen.flow_from_directory(
    TRAIN_DIR,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=32,
    class_mode='categorical',
    subset='validation'
)

from sklearn.preprocessing import LabelEncoder
import pickle

# ✅ Create and fit the correct LabelEncoder instance
label_encoder = LabelEncoder()
label_encoder.fit(["Braid", "Curly", "Kinky", "Straight", "Wavy", "Locs"])  # Use your actual labels

# ✅ Save the trained LabelEncoder instance
with open('/content/drive/MyDrive/label_encoder.pkl', 'wb') as file:
    pickle.dump(label_encoder, file)

print("✅ Label encoder saved correctly!")

# Save label encoder mapping
#label_encoder = train_generator.class_indices
#with open('/content/drive/MyDrive/label_encoder.pkl', 'wb') as file:
   # pickle.dump(label_encoder, file)

#print("Label encoder saved successfully!")

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization
#define hairnet model
def create_hairnet_model(input_shape, num_classes):
    model = Sequential([
        Flatten(input_shape=input_shape),
        Dense(256, activation='relu'),
        BatchNormalization(),
        Dropout(0.5),

        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(0.5),

        Dense(num_classes, activation='softmax')
    ])
    return model

print(f"Labels found: {label_encoder.classes_}")
print(f"Total number of classes: {len(label_encoder.classes_)}")

import numpy as np
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder

# Define the selected 6 classes you want
selected_classes = ['Braid', 'Curly', 'Kinky', 'Locs', 'Straight', 'Wavy']

# Filter dataset to include only selected classes
filtered_indices = [i for i, label in enumerate(train_labels) if label in selected_classes]
filtered_images = np.array(train_image_paths)[filtered_indices]
filtered_labels = np.array(train_labels)[filtered_indices]

# Encode labels using LabelEncoder
label_encoder = LabelEncoder()
train_labels_encoded = label_encoder.fit_transform(filtered_labels)

# Convert labels to one-hot encoding (with 6 classes)
train_labels_one_hot = to_categorical(train_labels_encoded, num_classes=6)

# Check if shapes match
print(f"Train labels shape: {train_labels_one_hot.shape}")  # Should be (samples, 6)

"""USING EARLY STOPPER METHOD TO CORRECT OVERFITTING IN THE FINE TUNED MODEL"""

from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Early Stopping: Stop training if the validation loss does not improve for 5 epochs
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Learning Rate Scheduler: Reduce the learning rate if the validation loss does not improve for 5 epochs
lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)

hairnet = create_hairnet_model(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=6)
hairnet.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train HairNet model with early stopping and learning rate scheduler
history_hairnet = hairnet.fit(
    train_generator,
    validation_data=val_generator,
    epochs=10,
    callbacks=[early_stopping, lr_scheduler]
)

# Evaluate HairNet model after training
hairnet_eval = hairnet.evaluate(val_generator)
print(f"HairNet Accuracy (Before Fine-tuning): {hairnet_eval[1] * 100:.2f}%")

# Fine-tune HairNet with a lower learning rate
hairnet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

# Fine-tune HairNet model with early stopping and learning rate scheduler
history_hairnet_finetune = hairnet.fit(
    train_generator,
    validation_data=val_generator,
    epochs=5,
    callbacks=[early_stopping, lr_scheduler]
)

# Evaluate HairNet model after fine-tuning
hairnet_eval_finetune = hairnet.evaluate(val_generator)
print(f"HairNet Accuracy (After Fine-tuning): {hairnet_eval_finetune[1] * 100:.2f}%")

#hairnet = create_hairnet_model(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=6)
#hairnet.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train HairNet model
#history_hairnet = hairnet.fit(train_generator, validation_data=val_generator, epochs=10)

"""HairNet Model"""

# Evaluate HairNet model
#hairnet_eval = hairnet.evaluate(val_generator)
#print(f"HairNet Accuracy (Before Fine-tuning): {hairnet_eval[1] * 100:.2f}%")

"""CNN model"""

from tensorflow.keras.regularizers import l2

def create_cnn_model(input_shape, num_classes):
    model = Sequential([
        Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.001), input_shape=input_shape),
        MaxPooling2D((2, 2)),
        Dropout(0.25),

        Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
        MaxPooling2D((2, 2)),
        Dropout(0.25),

        Flatten(),
        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),
        Dropout(0.5),
        Dense(num_classes, activation='softmax')
    ])
    return model

num_classes = 6
cnn = create_cnn_model(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=num_classes)
cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

""""def create_cnn_model(input_shape, num_classes):
    model = Sequential([
        Conv2D(32, (3,3), activation='relu', kernel_regularizer=l2(0.001), input_shape=input_shape),
        MaxPooling2D((2,2)),
        Dropout(0.25),

        Conv2D(64, (3,3), activation='relu', kernel_regularizer=l2(0.001)),
        MaxPooling2D((2,2)),
        Dropout(0.25),

        Flatten(),
        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),
        Dropout(0.5),
        Dense(num_classes, activation='softmax')
    ])
    return model

num_classes = 6
cnn = create_cnn_model(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=num_classes)

cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])


# Train CNN model
history_cnn = cnn.fit(train_generator, validation_data=val_generator, epochs=10)

# Evaluate CNN model
cnn_eval = cnn.evaluate(val_generator)
print(f"CNN Accuracy (Before Fine-tuning): {cnn_eval[1] * 100:.2f}%")"""

# Train CNN model with early stopping and learning rate scheduler
history_cnn = cnn.fit(
    train_generator,
    validation_data=val_generator,
    epochs=10,
    callbacks=[early_stopping, lr_scheduler]
)

# Evaluate CNN model after training
cnn_eval = cnn.evaluate(val_generator)
print(f"CNN Accuracy (Before Fine-tuning): {cnn_eval[1] * 100:.2f}%")

# Fine-tune CNN with a lower learning rate
cnn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

# Fine-tune CNN model with early stopping and learning rate scheduler
history_cnn_finetune = cnn.fit(
    train_generator,
    validation_data=val_generator,
    epochs=5,
    callbacks=[early_stopping, lr_scheduler]
)

# Evaluate CNN model after fine-tuning
cnn_eval_finetune = cnn.evaluate(val_generator)
print(f"CNN Accuracy (After Fine-tuning): {cnn_eval_finetune[1] * 100:.2f}%")

"""Fine-Tunning the models

Hairnet
"""

# Fine-tune HairNet
"""hairnet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])
history_hairnet_finetune = hairnet.fit(train_generator, validation_data=val_generator, epochs=5)

hairnet_eval_finetune = hairnet.evaluate(val_generator)
print(f"HairNet Accuracy (After Fine-tuning): {hairnet_eval_finetune[1] * 100:.2f}%")"""

"""CNN"""

# Fine-tune CNN
"""cnn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])
history_cnn_finetune = cnn.fit(train_generator, validation_data=val_generator, epochs=5)

cnn_eval_finetune = cnn.evaluate(val_generator)
print(f"CNN Accuracy (After Fine-tuning): {cnn_eval_finetune[1] * 100:.2f}%")"""

"""Saving the models"""

# Save the trained models
cnn.save('/content/drive/MyDrive/cnn_model.h5')

hairnet.save('/content/drive/MyDrive/hairnet_model.h5')

print("Models saved successfully!")

"""VISUALIZATION OF THE MODELS

>

TRANSFER LEARNING FOR THE FINE TUNE OR CONFUSE THE MODEL BY INTRODUCE NOISE.
"""



import matplotlib.pyplot as plt

def plot_training_history(history, model_name):
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title(f'{model_name} Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

plot_training_history(history_hairnet, "HairNet")
plot_training_history(history_cnn, "CNN")

#plotting the loss function
# Define function
def plot_training_loss(history, model_name):
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title(f'{model_name} Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call function separately
plot_training_loss(history_hairnet, "HairNet")
plot_training_loss(history_cnn, "CNN")
plot_training_loss(history_hairnet_finetune, "HairNet (Fine-tuned)")
plot_training_loss(history_cnn_finetune, "CNN (Fine-tuned)")

"""From the loss graph it is clear that the validation for cnn is not reducing showing overfitting. So Dennis suggestion increase the dataset for the for the training.

TESTING HAIR MODEL
"""

#Importing all necessary libraries
from tensorflow.keras.preprocessing.image import load_img, img_to_array
import pickle

"""Step 2: Load the Saved Models"""

# Load the saved CNN model
cnn_model = tf.keras.models.load_model('/content/drive/MyDrive/cnn_model.h5')

# Load the saved HairNet model
hairnet_model = tf.keras.models.load_model('/content/drive/MyDrive/hairnet_model.h5')

# Load the label encoder
with open('/content/drive/MyDrive/label_encoder.pkl', 'rb') as file:
    label_encoder = pickle.load(file)

"""Step 3: Preprocess a Test Image"""

# Function to preprocess a single test image
def preprocess_test_image(image_path):
    IMG_HEIGHT, IMG_WIDTH = 128, 128  # Ensure it matches training dimensions
    img = load_img(image_path, target_size=(IMG_HEIGHT, IMG_WIDTH))  # Load and resize
    img_array = img_to_array(img) / 255.0  # Normalize pixel values
    img_array = np.expand_dims(img_array, axis=0)  # Expand dimensions for model input
    return img_array

"""Select a Test Image"""

from google.colab import files
import shutil

# Upload a file
uploaded = files.upload()

# Get the uploaded file name
test_image_path = list(uploaded.keys())[0]

# Move the uploaded file to a known location (optional)
shutil.move(test_image_path, "/content/" + test_image_path)

# Update test_image_path to reflect new location
test_image_path = "/content/" + test_image_path

print(f"Uploaded test image: {test_image_path}")

test_image = preprocess_test_image(test_image_path)

"""Step 5: Make Predictions"""

# Predict using CNN model
cnn_prediction = cnn_model.predict(test_image)
cnn_predicted_label = np.argmax(cnn_prediction, axis=1)

# Predict using HairNet model
hairnet_prediction = hairnet_model.predict(test_image)
hairnet_predicted_label = np.argmax(hairnet_prediction, axis=1)

"""Step 6: Decode the Predictions"""

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()

label_encoder.fit(["Braid", "Curly", "Kinky", "Straight", "Wavy", "Locs"])

# Ensure label_encoder is an instance of LabelEncoder
if not isinstance(label_encoder, LabelEncoder):
    label_encoder = LabelEncoder()
    label_encoder.fit(["Braid", "Curly", "Kinky", "Straight", "Wavy", "Locs"])  # Fit it with your hair type labels

# Now inverse transform should work
cnn_predicted_class = label_encoder.inverse_transform([cnn_predicted_label])  # Ensure input is a list
hairnet_predicted_class = label_encoder.inverse_transform([hairnet_predicted_label])

print(type(label_encoder))  # Should output: <class 'sklearn.preprocessing._label.LabelEncoder'>

# Decode predictions
# Convert the numerical predictions back to their respective class labels.

#defining true label
true_label = "Curly"

cnn_predicted_class = label_encoder.inverse_transform(cnn_predicted_label)
hairnet_predicted_class = label_encoder.inverse_transform(hairnet_predicted_label)

print(f"Predicted Hair Type (CNN): {cnn_predicted_class[0]}")
print(f"Predicted Hair Type (HairNet): {hairnet_predicted_class[0]}")

# Calculate accuracy
cnn_accuracy = 1 if cnn_predicted_class[0] == true_label else 0
hairnet_accuracy = 1 if hairnet_predicted_class[0] == true_label else 0

"""Step 7: Evaluate the Models on a Test Dataset"""

import os
import pickle

# Define the path to the test dataset
TEST_DIR = "/content/drive/MyDrive/Figaro-1k/GT/Testing"

 #✅ Step 1: Check if test directory exists
if os.path.exists(TEST_DIR):
    print("✅ Test directory exists.")
else:
    print("❌ Test directory NOT found! Check the path.")

# ✅ Step 2: Load test images and labels
test_image_paths = []
test_labels = []

# Loop through each labeled subfolder
for category in os.listdir(TEST_DIR):
    folder_path = os.path.join(TEST_DIR, category)
    if os.path.isdir(folder_path):  # Ensure it's a folder
        for file in os.listdir(folder_path):
            if file.lower().endswith(('.jpg', '.jpeg', '.png')):  # Filter image files
                test_image_paths.append(os.path.join(folder_path, file))
                test_labels.append(category)  # Assign folder name as label

# Convert to a DataFrame and save to CSV (for debugging if needed)
test_df = pd.DataFrame({"image_path": test_image_paths, "label": test_labels})
csv_path = "/content/drive/MyDrive/Figaro-1k/test_labels_fixed.csv"
test_df.to_csv(csv_path, index=False)

print(f"✅ Loaded {len(test_image_paths)} test images and {len(test_labels)} labels.")
print(test_df.head())

# ✅ Step 3: Load the label encoder (Ensure this file exists from training)
with open('/content/drive/MyDrive/label_encoder.pkl', 'rb') as file:
    label_encoder = pickle.load(file)

# ✅ Step 4: Encode the test labels using the same encoder from training
test_labels_encoded = label_encoder.transform(test_labels)

# ✅ Step 5: One-hot encode labels (for model evaluation)
test_labels_one_hot = tf.keras.utils.to_categorical(test_labels_encoded, num_classes=len(label_encoder.classes_))

# ✅ Step 6: Load and preprocess test images (Reshape & normalize for model input)
def load_and_preprocess_image(image_path, target_size=(128, 128)):  # Ensure it matches model input size
    image = cv2.imread(image_path)
    if image is None:
        print(f"⚠ Warning: Could not read image {image_path}")  # Debugging missing images
        return None
    image = cv2.resize(image, target_size)
    image = image / 255.0  # Normalize pixel values
    return image

# Convert test image paths to actual image tensors
test_images = np.array([load_and_preprocess_image(img) for img in test_image_paths if load_and_preprocess_image(img) is not None])

print(f"✅ Loaded {test_images.shape[0]} images for testing.")

## ✅ Step 7: Load the trained models
cnn_model = tf.keras.models.load_model('/content/drive/MyDrive/cnn_model.h5')
hairnet_model = tf.keras.models.load_model('/content/drive/MyDrive/hairnet_model.h5')

print("✅ Models and label encoder loaded successfully!")

# Load test labels from a CSV file (this CSV should have at least a 'label' column)
#test_labels_df = pd.read_csv("/content/drive/MyDrive/Figaro-1k/test_labels.csv")

#print("Test Labels DataFrame shape:", test_labels_df.shape)  # Should match number of test images

# Check the first few entries to confirm labels exist
#print(test_labels_df.head())

TEST_DIR = "/content/drive/MyDrive/Figaro-1k/GT"

if os.path.exists(TEST_DIR):
    print(f"✅ Test directory found: {TEST_DIR}")
else:
    raise ValueError("❌ Test directory NOT found! Check the path.")

print("📂 Subfolders inside TEST_DIR:", os.listdir(TEST_DIR))

# Scan test directory for images
test_image_paths = []
for root, dirs, files in os.walk(TEST_DIR):
    for file in files:
        if file.lower().endswith(('.jpg', '.jpeg', '.png')):
            test_image_paths.append(os.path.join(root, file))

# Print results
print(f"🔍 Found {len(test_image_paths)} test images.")
print("Sample paths:", test_image_paths[:5])

# Extract labels from folder names
test_labels = [os.path.basename(os.path.dirname(path)) for path in test_image_paths]

# Create a DataFrame
test_df = pd.DataFrame({"image_path": test_image_paths, "label": test_labels})

# Save to CSV
csv_path = "/content/drive/MyDrive/Figaro-1k/test_labels_fixed.csv"
test_df.to_csv(csv_path, index=False)

print(f"✅ Saved test labels CSV with {len(test_df)} entries.")
print(test_df.head())  # Display first few rows to check if labels are correct

# Load the labeled test dataset
test_labels_df = pd.read_csv("/content/drive/MyDrive/Figaro-1k/test_labels_fixed.csv")

# Load the original label encoder from training
with open('/content/drive/MyDrive/label_encoder.pkl', 'rb') as file:
    label_encoder = pickle.load(file)

print("Classes in trained Label Encoder:", label_encoder.classes_)


# Extract image paths & labels
test_image_paths = test_labels_df["image_path"].tolist()
test_labels = test_labels_df["label"].tolist()

print(f"✅ Loaded {len(test_image_paths)} test images from labeled dataset.")

# Encode labels (Ensure you're using the same LabelEncoder from training)
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
test_labels_encoded = label_encoder.fit_transform(test_labels)

# One-hot encode labels (for model evaluation)
test_labels_one_hot = tf.keras.utils.to_categorical(test_labels_encoded, num_classes=len(label_encoder.classes_))

# Function to load and preprocess images
import cv2
def load_and_preprocess_image(image_path, target_size=(128, 128)):
    image = cv2.imread(image_path)

    if image is None:
        print(f"⚠ Warning: Could not read image {image_path}")
        return None

    image = cv2.resize(image, target_size)  # Resize to fit model input
    image = image / 255.0  # Normalize pixel values
    return image

# Load and preprocess images
test_images = np.array([
    load_and_preprocess_image(img) for img in test_image_paths if load_and_preprocess_image(img) is not None
])

print(f"✅ Preprocessed {test_images.shape[0]} test images.")

# Ensure images are not empty
if test_images.shape[0] == 0:
    raise ValueError("❌ No test images available! Check image paths and preprocessing.")

# ✅ Ensure correct shape for model input
print(f"Test images shape: {test_images.shape}")  # Should be (N, 128, 128, 3)

# Predict using CNN model
cnn_predictions = cnn_model.predict(test_images)
cnn_predicted_labels = np.argmax(cnn_predictions, axis=1)

# Predict using HairNet model
hairnet_predictions = hairnet_model.predict(test_images)
hairnet_predicted_labels = np.argmax(hairnet_predictions, axis=1)

print("Classes in trained Label Encoder:", label_encoder.classes_)
print("Unique values in CNN Predictions:", np.unique(cnn_predicted_labels))
print("Unique values in HairNet Predictions:", np.unique(hairnet_predicted_labels))

# Convert numerical predictions back to class labels
# Map only known labels, replace unknown labels with "Unknown"
cnn_predicted_classes = [
    label_encoder.classes_[label] if label < len(label_encoder.classes_) else "Unknown"
    for label in cnn_predicted_labels
]

hairnet_predicted_classes = [
    label_encoder.classes_[label] if label < len(label_encoder.classes_) else "Unknown"
    for label in hairnet_predicted_labels
]

# Ensure the predicted list has the same length as test images
if len(cnn_predicted_classes) != len(test_labels_df):
    print(f"⚠ Warning: CNN predictions ({len(cnn_predicted_classes)}) do not match test labels ({len(test_labels_df)}).")
if len(hairnet_predicted_classes) != len(test_labels_df):
    print(f"⚠ Warning: HairNet predictions ({len(hairnet_predicted_classes)}) do not match test labels ({len(test_labels_df)}).")


# Add predictions to the test DataFrame
test_labels_df["CNN Prediction"] = cnn_predicted_classes
test_labels_df["HairNet Prediction"] = hairnet_predicted_classes

# Save the predictions
test_labels_df.to_csv("/content/drive/MyDrive/Figaro-1k/test_predictions_fixed.csv", index=False)

print("✅ Predictions saved successfully!")

from sklearn.metrics import accuracy_score

# Ensure that test labels are encoded the same way as during training
test_labels_encoded = label_encoder.transform(test_labels_df["label"])

# Compute accuracy for CNN model
cnn_accuracy = accuracy_score(test_labels_encoded, cnn_predicted_labels)
print(f"✅ CNN Model Test Accuracy: {cnn_accuracy * 100:.2f}%")

# Compute accuracy for HairNet model
hairnet_accuracy = accuracy_score(test_labels_encoded, hairnet_predicted_labels)
print(f"✅ HairNet Model Test Accuracy: {hairnet_accuracy * 100:.2f}%")

"""RECOMMENDATION MODEL WITH RANDOM FOREST"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.utils import resample
from collections import Counter

#Load the datasets
serum_df = pd.read_csv("/content/drive/MyDrive/product_dataset/serum.csv")
shampoo_df = pd.read_csv("/content/drive/MyDrive/product_dataset/shampoo_conditoner.csv")
conditioner_df = pd.read_csv("/content/drive/MyDrive/product_dataset/shampoo_data.csv")

"""Combining all of the datasets into one"""

# Combine all datasets into one
df = pd.concat([serum_df, shampoo_df, conditioner_df], ignore_index=True)
df.head()

# Remove extra spaces in column names
df.columns = df.columns.str.strip()

# Display available column names to confirm their actual names
print("Available columns:", df.columns.tolist())

# Keep only the relevant columns (Modify this based on your dataset structure)
relevant_columns = ['Product Name', 'Specification', 'HighLight', 'Product Cost']
df = df[relevant_columns].dropna()  # Drop rows with missing values in selected columns

# Clean the "Product Cost" column (remove ₹, $, etc.)
df['Product Cost'] = df['Product Cost'].replace(r'[^\d.]', '', regex=True)  # Remove non-numeric characters
df['Product Cost'] = pd.to_numeric(df['Product Cost'])  # Convert to float

from sklearn.preprocessing import LabelEncoder

# Encode categorical features (Hair Type, Hair Concern, Specification, and HighLight)
label_enc = LabelEncoder()
df['Specification'] = label_enc.fit_transform(df['Specification'].astype(str))
df['HighLight'] = label_enc.fit_transform(df['HighLight'].astype(str))

# 📌 Encode target variable (Product Name)
y_enc = LabelEncoder()
df['Product Name Encoded'] = y_enc.fit_transform(df['Product Name'])

# 📌 Save mapping of encoded labels to actual product names
label_mapping = {idx: product for idx, product in enumerate(y_enc.classes_)}

print("\nLabel Mapping (Encoded → Product Name):")
print(label_mapping)

# 📌 Balance the dataset by oversampling minority classes
grouped = df.groupby('Product Name Encoded')
max_samples = grouped.size().max()

balanced_df = grouped.apply(lambda x: resample(x, replace=True, n_samples=max_samples, random_state=42))
balanced_df = balanced_df.droplevel(0).reset_index(drop=True)

print("\nBalanced class distribution:\n", balanced_df['Product Name'].value_counts())

"""I NEED TO WORK FROM"""

#from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score

# --- STEP 1: Hair type extraction ---
def extract_hair_type(text):
    text = str(text).lower()
    types = ['curly', 'kinky', 'wavy', 'straight', 'braids', 'all hair types']
    for t in types:
        if t in text:
            return t
    return "unknown"

df['hair_type'] = df['HighLight'].apply(extract_hair_type)

# --- STEP 2: Encode categorical features ---
label_enc = LabelEncoder()
df['Specification'] = label_enc.fit_transform(df['Specification'].astype(str))
df['HighLight'] = label_enc.fit_transform(df['HighLight'].astype(str))
df['hair_type'] = label_enc.fit_transform(df['hair_type'].astype(str))

# --- STEP 3: Encode target variable ---
y_enc = LabelEncoder()
df['Product Name Encoded'] = y_enc.fit_transform(df['Product Name'])
label_mapping = {idx: product for idx, product in enumerate(y_enc.classes_)}
print("\nLabel Mapping (Encoded → Product Name):")
print(label_mapping)

# --- STEP 4: Balance the dataset ---
grouped = df.groupby('Product Name Encoded')
max_samples = grouped.size().max()
balanced_df = grouped.apply(lambda x: resample(x, replace=True, n_samples=max_samples, random_state=42))
balanced_df = balanced_df.droplevel(0).reset_index(drop=True)

# --- STEP 5: Define features and target ---
X = balanced_df[['Specification', 'HighLight', 'Product Cost', 'hair_type']]
y = balanced_df['Product Name Encoded']

# --- STEP 6: Train-test split ---
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# --- STEP 7: GridSearchCV for hyperparameter tuning ---
param_grid = {
    'n_estimators': [100, 150, 200],
    'max_depth': [10, 15, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

grid_search = GridSearchCV(RandomForestClassifier(random_state=42),
                           param_grid, cv=5, n_jobs=-1, verbose=1)

# Model initialization with regularization
'''rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,            # Control depth to prevent overfitting
    min_samples_split=5,     # Require more samples before splitting
    min_samples_leaf=2,      # Leaf must have at least 2 samples
    max_features='sqrt',     # Use sqrt of features at each split
    random_state=42
)'''

df.head()

grid_search.fit(X_train, y_train)

import joblib
joblib.dump(grid_search.best_estimator_, '/content/drive/MyDrive/optimized_rf_model.joblib')
print("✅ Saved best model to Google Drive.")

# --- STEP 8: Best model from GridSearch ---
print("Best Hyperparameters:", grid_search.best_params_)
best_rf_model = grid_search.best_estimator_

# --- STEP 9: Evaluate best model ---
y_pred = best_rf_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Test Accuracy:", accuracy)

from sklearn.metrics import precision_score, recall_score, f1_score

precision = precision_score(y_test, y_pred, average='macro')
recall = recall_score(y_test, y_pred, average='macro')
f1 = f1_score(y_test, y_pred, average='macro')

print(f"Precision: {precision:.3f}")
print(f"Recall: {recall:.3f}")
print(f"F1 Score: {f1:.3f}")

# --- STEP 10: Optional Cross-validation plot ---
train_scores = []
test_scores = []
cv_folds = 5

for fold in range(cv_folds):
    scores = cross_val_score(best_rf_model, X, y, cv=cv_folds, scoring='accuracy')
    test_scores = scores
    best_rf_model.fit(X_train, y_train)
    train_acc = best_rf_model.score(X_train, y_train)
    train_scores = [train_acc] * cv_folds  # Repeat for plot

# Plot
plt.figure(figsize=(4, 4))
plt.plot(range(1, cv_folds + 1), train_scores, label='Train Accuracy', marker='o', color='green')
plt.plot(range(1, cv_folds + 1), test_scores, label='Test Accuracy (CV Folds)', marker='x', color='red')
plt.xlabel('Cross-validation Folds')
plt.ylabel('Accuracy')
plt.title('Training and Test Accuracy Over Cross-validation Folds')
plt.legend()
plt.show()

import joblib
joblib.dump(grid_search.best_estimator_, 'optimized_rf_model.joblib')
print("Model saved as optimized_rf_model.joblib"

#can also save your label encoder if you're using it for decoding predictions
joblib.dump(y_enc, 'label_encoder.joblib')

from google.colab import files
files.download('optimized_rf_model.joblib')

"""The Old Code"""

# Cross-validation
cv_scores = cross_val_score(rf_model, X, y, cv=5, scoring='accuracy')
plt.figure(figsize=(6, 6))
plt.plot(np.arange(1, len(cv_scores) + 1), cv_scores, marker='o', color='b', label='Accuracy per fold')
plt.title('Cross-validation Accuracy Scores')
plt.xlabel('Fold Number')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

print("Cross-validation accuracy scores: ", cv_scores)
print("Average cross-validation accuracy: ", np.mean(cv_scores))

# Train and evaluate
rf_model.fit(X_train, y_train)
y_pred = rf_model.predict(X_test)
y_pred = [i if i in label_mapping else 0 for i in y_pred]
y_pred_labels = [label_mapping[label] for label in y_pred]

print("\nSample Predictions:")
for i in range(min(5, len(y_pred_labels))):
    print(f"Predicted: {y_pred_labels[i]}")

print("\nUpdated Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred, zero_division=1))

# Train vs Test Accuracy
plt.figure(figsize=(4, 4))
epochs = np.arange(1, len(cv_scores) + 1)
train_accuracy = accuracy_score(y_train, rf_model.predict(X_train))
train_accuracies = [train_accuracy] * len(cv_scores)

plt.plot(epochs, train_accuracies, marker='o', color='g', label='Train Accuracy')
plt.plot(epochs, cv_scores, marker='x', color='r', label='Test Accuracy (CV Folds)')
plt.title('Training and Test Accuracy Over Cross-validation Folds')
plt.xlabel('Cross-validation Folds')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Save model
with open('/content/drive/MyDrive/rf_model.pkl', 'wb') as file:
    pickle.dump(rf_model, file)

#Define features (X) and target (y)
'''X = balanced_df[['Specification', 'HighLight', 'Product Cost']]  # Features
y = balanced_df['Product Name Encoded']  # Target variable'''

# Split data into training and testing sets (80% train, 20% test)
'''X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)'''

from collections import Counter

# Print class distributions in train and test sets
'''print("\nClasses in Train Set:", Counter(y_train))
print("Classes in Test Set:", Counter(y_test))'''

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RandomizedSearchCV

# Initialize the base model
'''rf = RandomForestClassifier(random_state=42)

# Reduced hyperparameter space for efficiency
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

# Setup the randomized search
random_search = RandomizedSearchCV(
    estimator=rf,
    param_distributions=param_grid,
    n_iter=10,         # Try only 10 random combinations
    cv=3,              # 3-fold CV to reduce load
    verbose=1,
    random_state=42,
    n_jobs=1           # Use 1 core to avoid memory errors
)
# Fit the search model
random_search.fit(X, y)'''

# Output the best parameters
#print("✅ Best Parameters Found:", random_search.best_params_)

# Initialize the Random Forest model with optimized parameters
''''rf_model=RandomForestClassifier(
    n_estimators=100,
    max_depth=10,           # Try limiting tree depth
    min_samples_split=5,    # Require more samples to split a node
    min_samples_leaf=2,     # Require more samples at leaf
    max_features='sqrt',    # Use fewer features per split
    random_state=42
)'''

#rf_model = random_search.best_estimator_

# Perform cross-validation to evaluate the model
'''from sklearn.model_selection import cross_val_score
cv_scores = cross_val_score(rf_model, X, y, cv=5, scoring='accuracy')  # 5-fold cross-validation'''

# Plotting the cross-validation results (accuracy per fold)
'''plt.figure(figsize=(6, 6))
plt.plot(np.arange(1, len(cv_scores) + 1), cv_scores, marker='o', color='b', label='Accuracy per fold')
plt.title('Cross-validation Accuracy Scores')
plt.xlabel('Fold Number')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Print cross-validation results
print("Cross-validation accuracy scores: ", cv_scores)
print("Average cross-validation accuracy: ", np.mean(cv_scores))'''

# Train the model
#rf_model.fit(X_train, y_train)

# Make predictions
#y_pred = rf_model.predict(X_test)

# Ensure predictions are valid labels
'''valid_indices = [i for i in range(len(y_enc.classes_))]
y_pred = [i if i in valid_indices else 0 for i in y_pred]'''  # Convert unknown labels to a default valid index

# Print predicted class distribution
#print("Classes Predicted:", Counter(y_pred))

# 📌 Evaluate the model
'''accuracy = accuracy_score(y_test, y_pred)
print("\nUpdated Accuracy:", accuracy)
print("\nClassification Report:\n", classification_report(y_test, y_pred, zero_division=1))'''

# Use zero_division=1 to avoid warnings
#print("\nClassification Report:\n", classification_report(y_test, y_pred, zero_division=1))

# Encode target variable (Product Name)
'''y_enc = LabelEncoder()
df['Product Name'] = y_enc.fit_transform(df['Product Name'])'''  # Encode Product Names

# 📌 Convert predictions back to product names using the saved mapping
#y_pred_labels = [label_mapping[label] if label in label_mapping else "Unknown" for label in y_pred]

# Sample predictions
# 📌 Display sample predictions
'''print("\nSample Predictions:")
for i in range(min(5, len(y_pred_labels))):
    print(f"Predicted: {y_pred_labels[i]}")'''

# Print predicted class distribution
'''print("\nClasses Predicted:", Counter(y_pred))

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("\nUpdated Accuracy:", accuracy)
print("\nClassification Report:\n", classification_report(y_test, y_pred, zero_division=1))'''

# Plotting train vs test accuracy to check for overfitting (accuracy per fold vs model's performance on test set)
'''plt.figure(figsize=(4, 4))
epochs = np.arange(1, len(cv_scores) + 1)
train_accuracy = accuracy_score(y_train, rf_model.predict(X_train))
train_accuracies = [train_accuracy] * len(cv_scores)  # Assuming the model is stable across folds

plt.plot(epochs, train_accuracies, marker='o', color='g', label='Train Accuracy')
plt.plot(epochs, cv_scores, marker='x', color='r', label='Test Accuracy (CV Folds)')
plt.title('Training and Test Accuracy Over Cross-validation Folds')
plt.xlabel('Cross-validation Folds')
plt.ylabel('Accuracy')
plt.legend()
plt.show()'''

#save the rf model
'''import pickle
with open('/content/drive/MyDrive/rf_model.pkl', 'wb') as file:
    pickle.dump(rf_model, file)'''